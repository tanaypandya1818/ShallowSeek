{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, time, os\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pages.json', 'r') as f:\n",
    "    pages = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_footnotes(footnotes_soup: bs4.element.Tag):\n",
    "    notes = {}\n",
    "    \n",
    "    for fn in footnotes_soup.contents:\n",
    "        if type(fn) == bs4.element.NavigableString:\n",
    "            continue\n",
    "        \n",
    "        fn_text = fn.text.replace('\\n', ' ')\n",
    "        fn_number, *fn_content = fn_text.split()\n",
    "        \n",
    "        notes[fn_number] = ' '.join(fn_content)\n",
    "        \n",
    "    return notes\n",
    "\n",
    "# Various HTML elements contain usable text:\n",
    "# - raw strings\n",
    "# - italicized phrases\n",
    "# - greek text\n",
    "def is_text(tag):\n",
    "    if type(tag) == bs4.element.NavigableString:\n",
    "        return True\n",
    "    \n",
    "    if tag.name == 'i':\n",
    "        return True\n",
    "    \n",
    "    if tag.name == 'span' and 'greek' in tag.attrs['class']:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def scrape_chapter(chapter_data):\n",
    "    content = req.get(chapter_data['url'])\n",
    "    content_soup = BeautifulSoup(content.text)\n",
    "    \n",
    "    footnotes_section = content_soup.find(name='div', class_='footnotes')\n",
    "    footnotes = parse_footnotes(footnotes_section)\n",
    "    \n",
    "    content_text_section = content_soup.find(name='div', class_='text').contents\n",
    "    paragraphs = []\n",
    "    \n",
    "    paragraph_text = ''\n",
    "    # Compress excess whitespace from replacing newlines into a single space\n",
    "    big_ws_re = r'\\s\\s\\s*'\n",
    "\n",
    "    for c in content_text_section:\n",
    "        if not is_text(c):\n",
    "            if c.name == 'p' and len(c) == 0:\n",
    "                p_text = re.sub(big_ws_re, ' ', paragraph_text).strip()\n",
    "                paragraphs.append(p_text)\n",
    "                paragraph_text = ''\n",
    "            elif c.name == 'a' and c.attrs['href'] and 'note' in c.attrs['href']:\n",
    "                footnote_num = c.attrs['href'][5:]\n",
    "                paragraph_text += f'<@{footnote_num}>'\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Exclude whitespace strings\n",
    "        if not c.text.strip():\n",
    "            continue\n",
    "        \n",
    "        c_text = c.text.replace('\\n', ' ')\n",
    "        paragraph_text += c_text\n",
    "\n",
    "    return {'paragraphs': paragraphs, 'footnotes': footnotes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages = len(pages)\n",
    "block_size = 100\n",
    "\n",
    "block_offset = 0\n",
    "\n",
    "# Scrape and save chapters in blocks to serve as checkpoints\n",
    "for n_block in range(block_offset, (n_pages // block_size) + 1):\n",
    "    chapters_with_data = []\n",
    "    start = n_block * block_size\n",
    "    end = start + block_size\n",
    "    \n",
    "    print(f'Scraping block #{n_block + 1} ({start}-{end} of {n_pages})')\n",
    "    \n",
    "    for i, chapter in enumerate(pages[start:end]):\n",
    "        print(f'>> Scraping page {i}/{block_size} [{i + start}/{n_pages}] [{chapter[\"chapter_name\"]}, {chapter[\"book_title\"]}]', end='\\r')\n",
    "        info = scrape_chapter(chapter)\n",
    "        \n",
    "        chapters_with_data.append({**chapter, **info})\n",
    "        \n",
    "        print(' ' * 200, end='\\r')\n",
    "\n",
    "    print(f'Saving block #{n_block + 1}...', end='\\r')\n",
    "    with open(f'data/info-blocks/info_{start}-{end}.json', 'w') as f:\n",
    "        json.dump(chapters_with_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print('Saved!' + ' ' * 200)\n",
    "    \n",
    "    # Sleep briefly to avoid spamming the site and potentially getting rate-limited\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_blocks = os.listdir('data/info-blocks')\n",
    "\n",
    "all_chapters = []\n",
    "\n",
    "for blocks in info_blocks:\n",
    "    with open(f'data/info-blocks/{blocks}', 'r') as f:\n",
    "        all_chapters.extend(json.load(f))\n",
    "        \n",
    "with open(f'data/chapter-data.json', 'w') as f:\n",
    "    json.dump(all_chapters, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
