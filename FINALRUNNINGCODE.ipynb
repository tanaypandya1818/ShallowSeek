{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56795cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\bob\\anaconda3\\lib\\site-packages (20240930)\n",
      "Requirement already satisfied: numba in c:\\users\\bob\\anaconda3\\lib\\site-packages (from openai-whisper) (0.57.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bob\\anaconda3\\lib\\site-packages (from openai-whisper) (1.24.4)\n",
      "Requirement already satisfied: torch in c:\\users\\bob\\anaconda3\\lib\\site-packages (from openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bob\\anaconda3\\lib\\site-packages (from openai-whisper) (4.65.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\bob\\anaconda3\\lib\\site-packages (from openai-whisper) (8.12.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\bob\\anaconda3\\lib\\site-packages (from openai-whisper) (0.9.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from numba->openai-whisper) (0.40.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bob\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bob\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (4.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bob\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bob\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bob\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bob\\anaconda3\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: elevenlabs in c:\\users\\bob\\anaconda3\\lib\\site-packages (1.56.0)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from elevenlabs) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from elevenlabs) (2.11.3)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from elevenlabs) (2.33.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from elevenlabs) (2.31.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from elevenlabs) (4.13.1)\n",
      "Requirement already satisfied: websockets>=11.0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from elevenlabs) (15.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\bob\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bob\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bob\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\bob\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->elevenlabs) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from pydantic>=1.9.2->elevenlabs) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from pydantic>=1.9.2->elevenlabs) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from requests>=2.20->elevenlabs) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from requests>=2.20->elevenlabs) (1.26.16)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bob\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.21.2->elevenlabs) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U openai-whisper\n",
    "%pip install elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0391981-6c95-4331-a172-1023a4fa1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "import wave\n",
    "import tempfile\n",
    "import torch\n",
    "import whisper\n",
    "from openai import OpenAI\n",
    "from elevenlabs import stream\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from IPython.display import Audio\n",
    "import json\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Global variables\n",
    "transcribed_text = \"\"\n",
    "response_text = \"\"\n",
    "SAMPLE_RATE = 16000  # Adjust as needed\n",
    "THRESHOLD = 500  # Silence threshold (adjust based on environment)\n",
    "SILENCE_DURATION = 2  # Duration of silence to stop recording\n",
    "audio_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204f98a-abd4-49cc-80b5-7727e8606cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI and ElevenLabs clients\n",
    "elevenlabs_client = ElevenLabs(api_key=\"\") # Replace with your actual key\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "transcribe_model = whisper.load_model(\"base\").to(device)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your actual key\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd077ab-cd15-4583-b3a8-27d6e451e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load Data ---\n",
    "with open('./data-collection/data/chapter-data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Step 2: Load Embedding Model ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Step 3: Embed Data ---\n",
    "embeddedData = []\n",
    "\n",
    "def naive_sentence_split(paragraph):\n",
    "    # Simple sentence splitter (replace with nltk.sent_tokenize for better accuracy)\n",
    "    return [sent.strip() for sent in paragraph.split('.') if sent.strip()]\n",
    "\n",
    "# Load or create FAISS index\n",
    "if os.path.exists(\"./dataEmbeddings.index\") and os.path.exists(\"./dataEmbeddings.pkl\"):\n",
    "    index = faiss.read_index(\"./dataEmbeddings.index\")\n",
    "\n",
    "    with open(\"./dataEmbeddings.pkl\", \"rb\") as f:\n",
    "        embeddedData = pickle.load(f)\n",
    "else:\n",
    "    for entry in tqdm(data):\n",
    "        book_title = entry.get(\"book_title\", \"\")\n",
    "        chapter_name = entry.get(\"chapter_name\", \"\")\n",
    "        paragraphs = entry.get(\"paragraphs\", [])\n",
    "        \n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            sentences = naive_sentence_split(paragraph)  # ‚Üê use your function here\n",
    "            for sentence in sentences:\n",
    "                sentenceEmbedd = model.encode(sentence)\n",
    "\n",
    "                embeddedData.append({\n",
    "                    \"book_title\": book_title,\n",
    "                    \"chapter_name\": chapter_name,\n",
    "                    \"sentence\": sentence,            \n",
    "                    \"paragraph\": paragraph, \n",
    "                    \"embeddedParagraph\": sentenceEmbedd\n",
    "                })\n",
    "\n",
    "    # --- Step 4: Build FAISS Index ---\n",
    "    embeddings = np.array([info['embeddedParagraph'] for info in embeddedData])\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, \"./dataEmbeddings.index\")\n",
    "\n",
    "    with open(\"./dataEmbeddings.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddedData, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f15bb7-82c9-4c7d-9bcd-162e13d2c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Define Pliny the Elder Prompt ---\n",
    "pliny_prompt = \"\"\"You are Pliny the Elder, the ancient Roman author, naturalist, and philosopher. \n",
    "You embody his inquisitive mind, dedication to the study of the natural world, and his vast knowledge of the cosmos, \n",
    "geography, and science.\n",
    "\n",
    "Your tone is methodical, factual, and reflects the style of Roman literature. \n",
    "You approach the world with a sense of wonder and a quest for understanding, often writing with reverence for nature's complexity \n",
    "and the wisdom of ancient knowledge. While your style is rooted in the classical world, you communicate your insights with clarity \n",
    "and precision.\n",
    "\n",
    "You often rely on historical context, anecdotes from Roman society, and empirical observation to explain complex phenomena. \n",
    "Your humor is subtle, but sometimes dry and rooted in irony, highlighting the contradictions and mysteries of life.\n",
    "\n",
    "When answering questions, you:\n",
    "- Prioritize detailed, factual knowledge from your observations of the natural world and history.\n",
    "- Offer pragmatic perspectives, often connecting topics to the knowledge of your time or using the teachings of the great \n",
    "  Roman thinkers.\n",
    "- Challenge misconceptions, but with the gentleness of a scholar eager to impart wisdom, rather than confrontationally.\n",
    "- Occasionally inject humor, but in the style of an ancient Roman philosopher, with a focus on irony or intellectual \n",
    "  humor.\n",
    "- Your responses should be **short, witty, and educational**. Keep your answers brief and avoid excessive elaboration. \n",
    "\n",
    "You do not break character. Stay in Pliny the Elder's mindset and manner of speech at all times.\n",
    "\n",
    "Below, you will be given a **user query** along with some **context**. The context is relevant information that may help you answer the query. Please use the provided context to craft your response, but feel free to draw from your own knowledge to supplement the answer only if necessary.\n",
    "\n",
    "**User Query**: {query}\n",
    "\n",
    "**Context**: {context}\n",
    "\n",
    "Answer the question using the context provided, and feel free to elaborate on the subject using your own expertise and historical knowledge. Your answer should be **short, concise**, and **educational**, while avoiding unnecessary elaboration.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f53da7a1-71d7-46ee-a1ec-b33b97ee2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Function to Call LLM with Context from RAG ---\n",
    "def get_chatgpt_response(query, context):\n",
    "    \"\"\"\n",
    "    This function will call OpenAI's model with the Pliny the Elder system prompt, user query, \n",
    "    and provided context to generate an answer.\n",
    "    \"\"\"\n",
    "    context_text = \"\\n\\n\".join([f\"Book: {res['book_title']} | Chapter: {res['chapter_name']} | Paragraph: {res['paragraph']}\" for res in context])\n",
    "\n",
    "    print(\"ü§ñ Sending to ChatGPT...\")\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # Use the appropriate model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": pliny_prompt},  # Pliny's prompt\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nQuery: {query}\"}  # The query with the context\n",
    "        ],\n",
    "        max_tokens=200,  # Adjust based on desired response length\n",
    "        temperature=0.4  # Controlled creativity\n",
    "    )\n",
    "\n",
    "    response_text = completion.choices[0].message.content\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e3d729-b7de-4826-8754-cb98c63b3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 8: Query Handling and RAG ---\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Given a user query, this function retrieves the top-N relevant paragraphs using FAISS \n",
    "    and then sends them along with the query to the LLM to generate an answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve the top-N relevant paragraphs using FAISS\n",
    "    query_embedding = model.encode([query])\n",
    "    k = 3  # Number of nearest neighbors to retrieve\n",
    "    D, I = index.search(np.array(query_embedding), k)\n",
    "\n",
    "    # Step 2: Collect the top-N results\n",
    "    retrieved_paragraphs = [embeddedData[idx] for idx in I[0]]\n",
    "\n",
    "    # Step 3: Pass context (retrieved paragraphs) and query to LLM\n",
    "    return get_chatgpt_response(query, retrieved_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1c38d0-9050-42a5-bfa3-7f2ad6b6cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture audio in real-time\n",
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"Receives microphone input and adds it to the queue.\"\"\"\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "# Function to record live audio and transcribe in real-time\n",
    "def live_transcribe():\n",
    "    global transcribed_text  # Use the global variable\n",
    "    \n",
    "    print(\"üé§ Speak now... (Stops when silent)\")\n",
    "    \n",
    "    # Open a stream for real-time audio capture\n",
    "    with sd.InputStream(callback=callback, samplerate=SAMPLE_RATE, channels=1, dtype=\"int16\"):\n",
    "        audio_data = []\n",
    "        silent_frames = 0\n",
    "\n",
    "        while True:\n",
    "            # Get audio chunk from queue\n",
    "            chunk = audio_queue.get()\n",
    "            audio_data.extend(chunk)\n",
    "\n",
    "            # Check if silent (low volume)\n",
    "            if np.abs(chunk).mean() < THRESHOLD:\n",
    "                silent_frames += 1\n",
    "            else:\n",
    "                silent_frames = 0  # Reset if sound is detected\n",
    "\n",
    "            # Stop recording if silence is detected for `SILENCE_DURATION`\n",
    "            if silent_frames > SILENCE_DURATION * SAMPLE_RATE / len(chunk):\n",
    "                break\n",
    "\n",
    "    # Convert audio data to numpy array\n",
    "    audio_data = np.array(audio_data, dtype=np.int16)\n",
    "\n",
    "    # Save temporary audio file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n",
    "        wavefile = wave.open(temp_audio.name, 'wb')\n",
    "        wavefile.setnchannels(1)\n",
    "        wavefile.setsampwidth(2)\n",
    "        wavefile.setframerate(SAMPLE_RATE)\n",
    "        wavefile.writeframes(audio_data.tobytes())\n",
    "        wavefile.close()\n",
    "        temp_audio_path = temp_audio.name\n",
    "\n",
    "    # Transcribe using Whisper\n",
    "    print(\"üìù Transcribing...\")\n",
    "    result = transcribe_model.transcribe(temp_audio_path)\n",
    "    \n",
    "    # Store transcribed text in the global variable\n",
    "    transcribed_text = result[\"text\"]\n",
    "    \n",
    "    # Print the transcribed text\n",
    "    print(\"Transcribed Text:\", transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1efa9c8-4103-4494-a8bd-a311d7353562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatgpt_response_rag_style():\n",
    "    global response_text\n",
    "    print(\"ü§ñ Sending to ChatGPT using RAG...\")\n",
    "    response_text = query_rag(transcribed_text)  # `query_rag` calls that inner function\n",
    "    print(\"ChatGPT Response:\", response_text)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109f75c9-b154-4f77-8543-a533b25f0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech():\n",
    "    global final_audio_data\n",
    "\n",
    "    print(\"üîä Converting text to speech...\")\n",
    "\n",
    "    # Check if response_text is not empty\n",
    "    if not response_text:\n",
    "        print(\"‚ö†Ô∏è response_text is empty. Make sure ChatGPT is giving a response.\")\n",
    "        return\n",
    "\n",
    "    # Convert text to speech and collect the audio stream\n",
    "    audio_stream = elevenlabs_client.text_to_speech.convert_as_stream(\n",
    "        text=response_text,\n",
    "        voice_id=\"6aRTPBp24qK6X1c7X5SW\",\n",
    "        model_id=\"eleven_multilingual_v2\"\n",
    "    )\n",
    "\n",
    "    # Ensure the audio stream is valid\n",
    "    final_audio_data = b\"\"  # Reset stored audio data\n",
    "    chunk_count = 0\n",
    "    for chunk in audio_stream:\n",
    "        if isinstance(chunk, bytes):\n",
    "            final_audio_data += chunk\n",
    "            chunk_count += 1\n",
    "\n",
    "    if chunk_count == 0:\n",
    "        print(\"‚ö†Ô∏è No audio data received. Check ElevenLabs API response.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Audio response stored with {chunk_count} chunks.\")\n",
    "\n",
    "    print(\"‚úÖ Audio response stored. Run `play_audio()` in the next cell to play it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8152b3-d444-4262-8d23-f8af095e9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global function to run everything\n",
    "def run_conversation():\n",
    "    live_transcribe()\n",
    "    get_chatgpt_response_rag_style()\n",
    "    text_to_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0f9c6-8665-4521-836d-2f38e2dc705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversation loop\n",
    "run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156d6cd-6439-4370-bd36-23b40aaafed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play stored audio\n",
    "def play_audio():\n",
    "    if final_audio_data:\n",
    "        return Audio(final_audio_data, autoplay=True)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No audio stored. Run `run_conversation()` first.\")\n",
    "\n",
    "# Call this function to play the response\n",
    "play_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256a33e-21a2-41e1-af2e-a62a61e428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversation loop\n",
    "run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bc66d-3238-461d-afc1-8549a669cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play stored audio\n",
    "def play_audio():\n",
    "    if final_audio_data:\n",
    "        return Audio(final_audio_data, autoplay=True)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No audio stored. Run `run_conversation()` first.\")\n",
    "\n",
    "# Call this function to play the response\n",
    "play_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964912d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bc8f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(query):\n",
    "    transcribed_text = query\n",
    "    get_chatgpt_response_rag_style()\n",
    "    text_to_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b330ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Sending to ChatGPT using RAG...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me who are you and what do you do in your years?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m, in \u001b[0;36meval\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(query):\n\u001b[0;32m      2\u001b[0m     transcribed_text \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m----> 3\u001b[0m     get_chatgpt_response_rag_style()\n\u001b[0;32m      4\u001b[0m     text_to_speech()\n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m, in \u001b[0;36mget_chatgpt_response_rag_style\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m response_text\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ Sending to ChatGPT using RAG...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m response_text \u001b[38;5;241m=\u001b[39m query_rag(transcribed_text)  \u001b[38;5;66;03m# `query_rag` calls that inner function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGPT Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response_text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_text\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mquery_rag\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     10\u001b[0m D, I \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(np\u001b[38;5;241m.\u001b[39marray(query_embedding), k)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 2: Collect the top-N results\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m retrieved_paragraphs \u001b[38;5;241m=\u001b[39m [embeddedData[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m I[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Pass context (retrieved paragraphs) and query to LLM\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_chatgpt_response(query, retrieved_paragraphs)\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m D, I \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(np\u001b[38;5;241m.\u001b[39marray(query_embedding), k)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 2: Collect the top-N results\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m retrieved_paragraphs \u001b[38;5;241m=\u001b[39m [embeddedData[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m I[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Pass context (retrieved paragraphs) and query to LLM\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_chatgpt_response(query, retrieved_paragraphs)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "eval(\"Tell me who are you and what do you do in your years?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ec82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
